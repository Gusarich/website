[
  {
    "model": "GPT-4o Search Preview",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 59.2,
        "Relevance": 81.0,
        "Completeness": 84.39999999999999,
        "Clarity": 96.6,
        "Conciseness": 55.4,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 71.654,
      "perfect_faithfulness_pct": 8.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 60,
        "Relevance": 81.42857142857142,
        "Completeness": 86.12244897959184,
        "Clarity": 96.53061224489797,
        "Conciseness": 55.30612244897959,
        "Self-Containedness": 99.38775510204081
      },
      "overall": 72.39795918367346,
      "perfect_faithfulness_pct": 8.16326530612245
    }
  },
  {
    "model": "GPT-4o mini Search Preview",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 54.6,
        "Relevance": 79.6,
        "Completeness": 80.19999999999999,
        "Clarity": 96.19999999999999,
        "Conciseness": 53.0,
        "Self-Containedness": 99.2
      },
      "overall": 68.346,
      "perfect_faithfulness_pct": 3.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 54.6,
        "Relevance": 79.6,
        "Completeness": 80.19999999999999,
        "Clarity": 96.19999999999999,
        "Conciseness": 53.0,
        "Self-Containedness": 99.2
      },
      "overall": 68.346,
      "perfect_faithfulness_pct": 3.0
    }
  },
  {
    "model": "Sonar Pro",
    "rejection_rate": 6.0,
    "all": {
      "scores": {
        "Faithfulness": 48.6,
        "Relevance": 74.0,
        "Completeness": 62.400000000000006,
        "Clarity": 86.19999999999999,
        "Conciseness": 50.0,
        "Self-Containedness": 96.8
      },
      "overall": 60.34,
      "perfect_faithfulness_pct": 11.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 51.27659574468085,
        "Relevance": 76.59574468085107,
        "Completeness": 66.38297872340425,
        "Clarity": 86.80851063829786,
        "Conciseness": 51.06382978723404,
        "Self-Containedness": 97.65957446808511
      },
      "overall": 62.92340425531915,
      "perfect_faithfulness_pct": 11.702127659574469
    }
  },
  {
    "model": "Sonar Reasoning Pro",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 46.8,
        "Relevance": 76.0,
        "Completeness": 63.2,
        "Clarity": 89.60000000000001,
        "Conciseness": 49.0,
        "Self-Containedness": 98.4
      },
      "overall": 60.29,
      "perfect_faithfulness_pct": 8.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 47.55102040816326,
        "Relevance": 76.53061224489795,
        "Completeness": 64.48979591836735,
        "Clarity": 90.0,
        "Conciseness": 49.183673469387756,
        "Self-Containedness": 98.36734693877551
      },
      "overall": 61.00612244897959,
      "perfect_faithfulness_pct": 8.16326530612245
    }
  },
  {
    "model": "Sonar Deep Research",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 38.0,
        "Relevance": 68.8,
        "Completeness": 82.6,
        "Clarity": 89.39999999999999,
        "Conciseness": 22.799999999999997,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 56.564,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 38.0,
        "Relevance": 68.8,
        "Completeness": 82.6,
        "Clarity": 89.39999999999999,
        "Conciseness": 22.799999999999997,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 56.564,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "GPT-4.5 Preview",
    "rejection_rate": 6.0,
    "all": {
      "scores": {
        "Faithfulness": 34.4,
        "Relevance": 94.80000000000001,
        "Completeness": 35.6,
        "Clarity": 98.0,
        "Conciseness": 81.0,
        "Self-Containedness": 98.80000000000001
      },
      "overall": 56.342000000000006,
      "perfect_faithfulness_pct": 9.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 35.744680851063826,
        "Relevance": 95.53191489361701,
        "Completeness": 37.87234042553192,
        "Clarity": 97.87234042553192,
        "Conciseness": 80.42553191489361,
        "Self-Containedness": 99.36170212765957
      },
      "overall": 57.47021276595744,
      "perfect_faithfulness_pct": 9.574468085106384
    }
  },
  {
    "model": "GPT-4.1 mini (2025-04-14)",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 27.400000000000002,
        "Relevance": 91.4,
        "Completeness": 32.599999999999994,
        "Clarity": 96.8,
        "Conciseness": 73.60000000000001,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 51.388000000000005,
      "perfect_faithfulness_pct": 4.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 27.755102040816325,
        "Relevance": 91.63265306122449,
        "Completeness": 33.265306122448976,
        "Clarity": 96.73469387755102,
        "Conciseness": 73.6734693877551,
        "Self-Containedness": 99.38775510204081
      },
      "overall": 51.714285714285715,
      "perfect_faithfulness_pct": 4.081632653061225
    }
  },
  {
    "model": "GPT-3.5 Turbo",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 17.0,
        "Relevance": 95.8,
        "Completeness": 23.599999999999998,
        "Clarity": 99.39999999999999,
        "Conciseness": 83.2,
        "Self-Containedness": 96.0
      },
      "overall": 46.72,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 17.0,
        "Relevance": 95.8,
        "Completeness": 23.599999999999998,
        "Clarity": 99.39999999999999,
        "Conciseness": 83.2,
        "Self-Containedness": 96.0
      },
      "overall": 46.72,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "o4-mini",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 20.4,
        "Relevance": 88.4,
        "Completeness": 26.6,
        "Clarity": 98.0,
        "Conciseness": 68.4,
        "Self-Containedness": 97.4
      },
      "overall": 46.218,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 21.041666666666664,
        "Relevance": 91.875,
        "Completeness": 27.708333333333336,
        "Clarity": 97.91666666666666,
        "Conciseness": 68.95833333333333,
        "Self-Containedness": 97.70833333333334
      },
      "overall": 47.44375,
      "perfect_faithfulness_pct": 1.0416666666666665
    }
  },
  {
    "model": "GPT-4.1 nano (2025-04-14)",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 19.6,
        "Relevance": 91.0,
        "Completeness": 23.799999999999997,
        "Clarity": 96.6,
        "Conciseness": 67.6,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 45.78,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 20,
        "Relevance": 91.875,
        "Completeness": 24.791666666666664,
        "Clarity": 96.45833333333334,
        "Conciseness": 66.875,
        "Self-Containedness": 99.79166666666666
      },
      "overall": 46.264583333333334,
      "perfect_faithfulness_pct": 1.0416666666666665
    }
  },
  {
    "model": "Gemini 2.5 Pro Preview (2025-06-05)",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 24.0,
        "Relevance": 78.60000000000001,
        "Completeness": 28.799999999999997,
        "Clarity": 98.6,
        "Conciseness": 41.6,
        "Self-Containedness": 96.4
      },
      "overall": 44.410000000000004,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 24.0,
        "Relevance": 78.60000000000001,
        "Completeness": 28.799999999999997,
        "Clarity": 98.6,
        "Conciseness": 41.6,
        "Self-Containedness": 96.4
      },
      "overall": 44.410000000000004,
      "perfect_faithfulness_pct": 1.0
    }
  },
  {
    "model": "GPT-4o (2024-08-06)",
    "rejection_rate": 5.0,
    "all": {
      "scores": {
        "Faithfulness": 21.8,
        "Relevance": 80.60000000000001,
        "Completeness": 22.200000000000003,
        "Clarity": 97.0,
        "Conciseness": 60.599999999999994,
        "Self-Containedness": 98.2
      },
      "overall": 43.904,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 22.31578947368421,
        "Relevance": 81.47368421052632,
        "Completeness": 23.368421052631582,
        "Clarity": 96.84210526315789,
        "Conciseness": 60.8421052631579,
        "Self-Containedness": 98.10526315789474
      },
      "overall": 44.52421052631579,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 2.5 Pro Preview (2025-05-06)",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 23.2,
        "Relevance": 77.0,
        "Completeness": 28.799999999999997,
        "Clarity": 93.6,
        "Conciseness": 46.8,
        "Self-Containedness": 96.4
      },
      "overall": 43.744,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 23.434343434343432,
        "Relevance": 77.77777777777777,
        "Completeness": 29.090909090909093,
        "Clarity": 93.53535353535352,
        "Conciseness": 46.26262626262626,
        "Self-Containedness": 96.36363636363637
      },
      "overall": 44.01414141414141,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Grok 2",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 18.2,
        "Relevance": 88.80000000000001,
        "Completeness": 18.799999999999997,
        "Clarity": 98.2,
        "Conciseness": 60.199999999999996,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 43.404,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 18.2,
        "Relevance": 88.80000000000001,
        "Completeness": 18.799999999999997,
        "Clarity": 98.2,
        "Conciseness": 60.199999999999996,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 43.404,
      "perfect_faithfulness_pct": 1.0
    }
  },
  {
    "model": "GPT-4o mini (2024-07-18)",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 19.2,
        "Relevance": 86.6,
        "Completeness": 19.0,
        "Clarity": 96.0,
        "Conciseness": 60.4,
        "Self-Containedness": 97.8
      },
      "overall": 43.262,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 19.387755102040817,
        "Relevance": 86.73469387755102,
        "Completeness": 19.387755102040817,
        "Clarity": 95.91836734693878,
        "Conciseness": 60.816326530612244,
        "Self-Containedness": 97.75510204081633
      },
      "overall": 43.465306122448986,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 2.0 Flash",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 18.2,
        "Relevance": 79.80000000000001,
        "Completeness": 25.2,
        "Clarity": 96.0,
        "Conciseness": 62.0,
        "Self-Containedness": 96.6
      },
      "overall": 42.644000000000005,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 18.571428571428573,
        "Relevance": 80.81632653061223,
        "Completeness": 25.714285714285715,
        "Clarity": 95.91836734693878,
        "Conciseness": 62.857142857142854,
        "Self-Containedness": 96.53061224489797
      },
      "overall": 43.159183673469386,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "GPT-4.1 (2025-04-14)",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 18.6,
        "Relevance": 80.19999999999999,
        "Completeness": 24.0,
        "Clarity": 95.8,
        "Conciseness": 59.800000000000004,
        "Self-Containedness": 97.8
      },
      "overall": 42.556000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 18.78787878787879,
        "Relevance": 81.01010101010101,
        "Completeness": 24.242424242424242,
        "Clarity": 95.75757575757576,
        "Conciseness": 59.39393939393939,
        "Self-Containedness": 98.18181818181819
      },
      "overall": 42.82626262626263,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Grok 3 Beta",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 20.8,
        "Relevance": 78.8,
        "Completeness": 22.400000000000002,
        "Clarity": 95.60000000000001,
        "Conciseness": 52.599999999999994,
        "Self-Containedness": 98.4
      },
      "overall": 42.478,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 20.8,
        "Relevance": 78.8,
        "Completeness": 22.400000000000002,
        "Clarity": 95.60000000000001,
        "Conciseness": 52.599999999999994,
        "Self-Containedness": 98.4
      },
      "overall": 42.478,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 2.5 Pro",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 19.6,
        "Relevance": 78.8,
        "Completeness": 25.8,
        "Clarity": 97.8,
        "Conciseness": 40.4,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 41.754,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 19.6,
        "Relevance": 78.8,
        "Completeness": 25.8,
        "Clarity": 97.8,
        "Conciseness": 40.4,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 41.754,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Acree Caller Large",
    "rejection_rate": 11.0,
    "all": {
      "scores": {
        "Faithfulness": 15.600000000000001,
        "Relevance": 82.0,
        "Completeness": 14.6,
        "Clarity": 97.0,
        "Conciseness": 67.8,
        "Self-Containedness": 97.8
      },
      "overall": 40.518,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 16.179775280898877,
        "Relevance": 86.51685393258427,
        "Completeness": 16.179775280898877,
        "Clarity": 97.52808988764045,
        "Conciseness": 68.98876404494382,
        "Self-Containedness": 98.65168539325843
      },
      "overall": 42.1123595505618,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Claude 3.5 Sonnet",
    "rejection_rate": 56.99999999999999,
    "all": {
      "scores": {
        "Faithfulness": 20,
        "Relevance": 75.0,
        "Completeness": 7.800000000000001,
        "Clarity": 97.6,
        "Conciseness": 69.4,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 40.076,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 25.581395348837212,
        "Relevance": 72.09302325581396,
        "Completeness": 18.13953488372093,
        "Clarity": 94.4186046511628,
        "Conciseness": 60.46511627906977,
        "Self-Containedness": 99.06976744186046
      },
      "overall": 43.0093023255814,
      "perfect_faithfulness_pct": 2.3255813953488373
    }
  },
  {
    "model": "DeepSeek V3",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 18.6,
        "Relevance": 74.4,
        "Completeness": 18.2,
        "Clarity": 97.2,
        "Conciseness": 45.0,
        "Self-Containedness": 98.4
      },
      "overall": 39.432,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 18.78787878787879,
        "Relevance": 75.15151515151516,
        "Completeness": 18.383838383838384,
        "Clarity": 97.17171717171718,
        "Conciseness": 45.25252525252525,
        "Self-Containedness": 98.3838383838384
      },
      "overall": 39.71515151515152,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "o1-pro",
    "rejection_rate": 14.000000000000002,
    "all": {
      "scores": {
        "Faithfulness": 20.4,
        "Relevance": 69.6,
        "Completeness": 20.4,
        "Clarity": 97.4,
        "Conciseness": 40.199999999999996,
        "Self-Containedness": 96.6
      },
      "overall": 39.302,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 23.02325581395349,
        "Relevance": 73.95348837209302,
        "Completeness": 23.720930232558143,
        "Clarity": 96.9767441860465,
        "Conciseness": 41.395348837209305,
        "Self-Containedness": 96.74418604651163
      },
      "overall": 42.009302325581395,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "GPT-4",
    "rejection_rate": 31.0,
    "all": {
      "scores": {
        "Faithfulness": 17.0,
        "Relevance": 73.0,
        "Completeness": 14.6,
        "Clarity": 96.8,
        "Conciseness": 66.8,
        "Self-Containedness": 96.19999999999999
      },
      "overall": 39.216,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 21.73913043478261,
        "Relevance": 73.04347826086956,
        "Completeness": 21.159420289855074,
        "Clarity": 95.3623188405797,
        "Conciseness": 60.57971014492754,
        "Self-Containedness": 95.3623188405797
      },
      "overall": 41.9768115942029,
      "perfect_faithfulness_pct": 1.4492753623188406
    }
  },
  {
    "model": "Gemini 1.5 Flash 8B",
    "rejection_rate": 33.0,
    "all": {
      "scores": {
        "Faithfulness": 16.4,
        "Relevance": 72.6,
        "Completeness": 15.600000000000001,
        "Clarity": 96.6,
        "Conciseness": 68.4,
        "Self-Containedness": 96.8
      },
      "overall": 39.162,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 17.611940298507463,
        "Relevance": 78.50746268656717,
        "Completeness": 23.28358208955224,
        "Clarity": 95.22388059701493,
        "Conciseness": 71.34328358208955,
        "Self-Containedness": 96.11940298507463
      },
      "overall": 42.36119402985075,
      "perfect_faithfulness_pct": 1.4925373134328357
    }
  },
  {
    "model": "Gemini 2.0 Flash Lite",
    "rejection_rate": 5.0,
    "all": {
      "scores": {
        "Faithfulness": 15.8,
        "Relevance": 76.6,
        "Completeness": 11.799999999999999,
        "Clarity": 93.80000000000001,
        "Conciseness": 57.800000000000004,
        "Self-Containedness": 98.4
      },
      "overall": 38.118,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 16.0,
        "Relevance": 79.57894736842105,
        "Completeness": 12.421052631578949,
        "Clarity": 93.47368421052633,
        "Conciseness": 58.31578947368421,
        "Self-Containedness": 98.3157894736842
      },
      "overall": 38.92631578947369,
      "perfect_faithfulness_pct": 1.0526315789473684
    }
  },
  {
    "model": "MiniMax-01",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 18.0,
        "Relevance": 73.8,
        "Completeness": 17.2,
        "Clarity": 88.2,
        "Conciseness": 39.6,
        "Self-Containedness": 93.80000000000001
      },
      "overall": 37.716,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 18.0,
        "Relevance": 73.8,
        "Completeness": 17.2,
        "Clarity": 88.2,
        "Conciseness": 39.6,
        "Self-Containedness": 93.80000000000001
      },
      "overall": 37.716,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Command A",
    "rejection_rate": 3.0,
    "all": {
      "scores": {
        "Faithfulness": 16.599999999999998,
        "Relevance": 71.6,
        "Completeness": 17.4,
        "Clarity": 95.19999999999999,
        "Conciseness": 41.2,
        "Self-Containedness": 96.8
      },
      "overall": 37.373999999999995,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 16.701030927835053,
        "Relevance": 72.16494845360825,
        "Completeness": 17.938144329896907,
        "Clarity": 95.05154639175258,
        "Conciseness": 40.824742268041234,
        "Self-Containedness": 97.31958762886597
      },
      "overall": 37.608247422680414,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "DeepSeek R1",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 16.4,
        "Relevance": 72.2,
        "Completeness": 16.0,
        "Clarity": 94.80000000000001,
        "Conciseness": 39.0,
        "Self-Containedness": 99.0
      },
      "overall": 37.036,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 16.4,
        "Relevance": 72.2,
        "Completeness": 16.0,
        "Clarity": 94.80000000000001,
        "Conciseness": 39.0,
        "Self-Containedness": 99.0
      },
      "overall": 37.036,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Claude 3 Opus",
    "rejection_rate": 52.0,
    "all": {
      "scores": {
        "Faithfulness": 16.8,
        "Relevance": 71.8,
        "Completeness": 3.8,
        "Clarity": 98.2,
        "Conciseness": 65.6,
        "Self-Containedness": 97.8
      },
      "overall": 37.004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 22.083333333333336,
        "Relevance": 70.0,
        "Completeness": 7.916666666666666,
        "Clarity": 97.08333333333334,
        "Conciseness": 58.33333333333333,
        "Self-Containedness": 96.66666666666666
      },
      "overall": 39.141666666666666,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen-Plus",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 15.600000000000001,
        "Relevance": 72.6,
        "Completeness": 15.600000000000001,
        "Clarity": 96.4,
        "Conciseness": 39.0,
        "Self-Containedness": 98.80000000000001
      },
      "overall": 36.79,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.600000000000001,
        "Relevance": 72.6,
        "Completeness": 15.600000000000001,
        "Clarity": 96.4,
        "Conciseness": 39.0,
        "Self-Containedness": 98.80000000000001
      },
      "overall": 36.79,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 2.5 Flash Preview (2025-04-17)",
    "rejection_rate": 5.0,
    "all": {
      "scores": {
        "Faithfulness": 15.0,
        "Relevance": 74.0,
        "Completeness": 14.399999999999999,
        "Clarity": 95.60000000000001,
        "Conciseness": 41.6,
        "Self-Containedness": 98.4
      },
      "overall": 36.698,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.789473684210527,
        "Relevance": 76.84210526315789,
        "Completeness": 15.157894736842106,
        "Clarity": 95.36842105263159,
        "Conciseness": 41.89473684210526,
        "Self-Containedness": 98.3157894736842
      },
      "overall": 37.760000000000005,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Devstral Small",
    "rejection_rate": 5.0,
    "all": {
      "scores": {
        "Faithfulness": 13.600000000000001,
        "Relevance": 75.0,
        "Completeness": 12.8,
        "Clarity": 90.60000000000001,
        "Conciseness": 65.4,
        "Self-Containedness": 76.8
      },
      "overall": 36.648,
      "perfect_faithfulness_pct": 2.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.105263157894736,
        "Relevance": 77.89473684210526,
        "Completeness": 13.473684210526315,
        "Clarity": 90.10526315789474,
        "Conciseness": 66.94736842105263,
        "Self-Containedness": 75.57894736842105
      },
      "overall": 37.61263157894737,
      "perfect_faithfulness_pct": 2.1052631578947367
    }
  },
  {
    "model": "Llama 3.3 70B Q4_0",
    "rejection_rate": 12.0,
    "all": {
      "scores": {
        "Faithfulness": 14.8,
        "Relevance": 69.6,
        "Completeness": 13.799999999999999,
        "Clarity": 93.80000000000001,
        "Conciseness": 59.6,
        "Self-Containedness": 94.0
      },
      "overall": 36.622,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.909090909090908,
        "Relevance": 73.63636363636363,
        "Completeness": 15.681818181818182,
        "Clarity": 93.40909090909092,
        "Conciseness": 58.40909090909091,
        "Self-Containedness": 94.77272727272727
      },
      "overall": 38.179545454545455,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Nova Pro 1.0",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 10.600000000000001,
        "Relevance": 79.6,
        "Completeness": 11.399999999999999,
        "Clarity": 96.6,
        "Conciseness": 54.2,
        "Self-Containedness": 99.2
      },
      "overall": 36.274,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.600000000000001,
        "Relevance": 79.6,
        "Completeness": 11.399999999999999,
        "Clarity": 96.6,
        "Conciseness": 54.2,
        "Self-Containedness": 99.2
      },
      "overall": 36.274,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "MiniMax M1",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 16.8,
        "Relevance": 69.2,
        "Completeness": 14.399999999999999,
        "Clarity": 96.19999999999999,
        "Conciseness": 36.800000000000004,
        "Self-Containedness": 97.8
      },
      "overall": 36.236000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 17.291666666666668,
        "Relevance": 70.20833333333333,
        "Completeness": 15.0,
        "Clarity": 96.25,
        "Conciseness": 37.083333333333336,
        "Self-Containedness": 97.91666666666666
      },
      "overall": 36.79375,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q8_0",
    "rejection_rate": 41.0,
    "all": {
      "scores": {
        "Faithfulness": 17.0,
        "Relevance": 66.8,
        "Completeness": 9.6,
        "Clarity": 94.0,
        "Conciseness": 55.0,
        "Self-Containedness": 95.0
      },
      "overall": 36.018,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 20,
        "Relevance": 74.23728813559322,
        "Completeness": 16.271186440677965,
        "Clarity": 91.1864406779661,
        "Conciseness": 43.05084745762712,
        "Self-Containedness": 95.59322033898304
      },
      "overall": 39.0406779661017,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Command R (08-2024)",
    "rejection_rate": 10.0,
    "all": {
      "scores": {
        "Faithfulness": 15.600000000000001,
        "Relevance": 69.6,
        "Completeness": 11.6,
        "Clarity": 97.2,
        "Conciseness": 46.2,
        "Self-Containedness": 96.8
      },
      "overall": 35.97,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 16.22222222222222,
        "Relevance": 74.0,
        "Completeness": 12.88888888888889,
        "Clarity": 96.88888888888889,
        "Conciseness": 46.88888888888889,
        "Self-Containedness": 96.44444444444444
      },
      "overall": 37.37777777777778,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "GPT-4 Turbo",
    "rejection_rate": 7.000000000000001,
    "all": {
      "scores": {
        "Faithfulness": 19.2,
        "Relevance": 59.400000000000006,
        "Completeness": 15.4,
        "Clarity": 95.60000000000001,
        "Conciseness": 43.6,
        "Self-Containedness": 97.6
      },
      "overall": 35.964000000000006,
      "perfect_faithfulness_pct": 2.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 19.78494623655914,
        "Relevance": 60,
        "Completeness": 16.559139784946236,
        "Clarity": 95.91397849462366,
        "Conciseness": 44.08602150537634,
        "Self-Containedness": 97.41935483870968
      },
      "overall": 36.60645161290323,
      "perfect_faithfulness_pct": 2.1505376344086025
    }
  },
  {
    "model": "Claude 3.7 Sonnet",
    "rejection_rate": 20.0,
    "all": {
      "scores": {
        "Faithfulness": 18.6,
        "Relevance": 60,
        "Completeness": 14.0,
        "Clarity": 97.0,
        "Conciseness": 46.6,
        "Self-Containedness": 98.2
      },
      "overall": 35.888000000000005,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 21.5,
        "Relevance": 67.5,
        "Completeness": 17.5,
        "Clarity": 96.25,
        "Conciseness": 50.5,
        "Self-Containedness": 98.5
      },
      "overall": 39.5525,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 4 Maverick",
    "rejection_rate": 31.0,
    "all": {
      "scores": {
        "Faithfulness": 14.6,
        "Relevance": 68.2,
        "Completeness": 13.4,
        "Clarity": 87.6,
        "Conciseness": 60,
        "Self-Containedness": 92.6
      },
      "overall": 35.732,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.362318840579709,
        "Relevance": 71.8840579710145,
        "Completeness": 19.420289855072465,
        "Clarity": 82.02898550724638,
        "Conciseness": 55.65217391304348,
        "Self-Containedness": 89.85507246376812
      },
      "overall": 37.118840579710145,
      "perfect_faithfulness_pct": 1.4492753623188406
    }
  },
  {
    "model": "o3-mini",
    "rejection_rate": 17.0,
    "all": {
      "scores": {
        "Faithfulness": 15.600000000000001,
        "Relevance": 71.8,
        "Completeness": 13.600000000000001,
        "Clarity": 92.4,
        "Conciseness": 36.2,
        "Self-Containedness": 93.80000000000001
      },
      "overall": 35.644,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 17.349397590361445,
        "Relevance": 77.34939759036145,
        "Completeness": 16.385542168674696,
        "Clarity": 90.84337349397589,
        "Conciseness": 35.42168674698795,
        "Self-Containedness": 93.97590361445782
      },
      "overall": 37.88433734939759,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "o1",
    "rejection_rate": 15.0,
    "all": {
      "scores": {
        "Faithfulness": 17.2,
        "Relevance": 63.4,
        "Completeness": 15.4,
        "Clarity": 97.0,
        "Conciseness": 38.8,
        "Self-Containedness": 96.6
      },
      "overall": 35.596000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 19.294117647058822,
        "Relevance": 70.35294117647058,
        "Completeness": 18.11764705882353,
        "Clarity": 96.47058823529412,
        "Conciseness": 40.94117647058823,
        "Self-Containedness": 96.47058823529412
      },
      "overall": 38.52705882352941,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen-Max",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 15.0,
        "Relevance": 68.2,
        "Completeness": 16.0,
        "Clarity": 95.60000000000001,
        "Conciseness": 37.2,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 35.548,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.416666666666668,
        "Relevance": 69.58333333333333,
        "Completeness": 16.666666666666668,
        "Clarity": 95.41666666666666,
        "Conciseness": 37.083333333333336,
        "Self-Containedness": 99.375
      },
      "overall": 36.110416666666666,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 1.5 Pro",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 12.6,
        "Relevance": 68.2,
        "Completeness": 12.4,
        "Clarity": 97.8,
        "Conciseness": 58.0,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 35.304,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.916666666666668,
        "Relevance": 69.16666666666667,
        "Completeness": 12.916666666666668,
        "Clarity": 97.70833333333334,
        "Conciseness": 59.375,
        "Self-Containedness": 95.0
      },
      "overall": 35.81666666666667,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Hermes 3 405B",
    "rejection_rate": 43.0,
    "all": {
      "scores": {
        "Faithfulness": 16.0,
        "Relevance": 65.19999999999999,
        "Completeness": 10.2,
        "Clarity": 96.4,
        "Conciseness": 51.0,
        "Self-Containedness": 96.8
      },
      "overall": 35.298,
      "perfect_faithfulness_pct": 2.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 19.29824561403509,
        "Relevance": 60,
        "Completeness": 17.894736842105264,
        "Clarity": 95.08771929824562,
        "Conciseness": 38.59649122807018,
        "Self-Containedness": 96.14035087719299
      },
      "overall": 36.14736842105263,
      "perfect_faithfulness_pct": 3.508771929824561
    }
  },
  {
    "model": "Hermes 3 70B",
    "rejection_rate": 26.0,
    "all": {
      "scores": {
        "Faithfulness": 15.2,
        "Relevance": 67.2,
        "Completeness": 11.200000000000001,
        "Clarity": 96.4,
        "Conciseness": 44.800000000000004,
        "Self-Containedness": 99.39999999999999
      },
      "overall": 35.162000000000006,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 17.2972972972973,
        "Relevance": 70.54054054054055,
        "Completeness": 15.135135135135137,
        "Clarity": 95.94594594594595,
        "Conciseness": 40,
        "Self-Containedness": 99.1891891891892
      },
      "overall": 37.10810810810811,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Mistral Medium 3",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 15.8,
        "Relevance": 66.2,
        "Completeness": 13.600000000000001,
        "Clarity": 95.19999999999999,
        "Conciseness": 37.2,
        "Self-Containedness": 98.2
      },
      "overall": 35.012,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.8,
        "Relevance": 66.2,
        "Completeness": 13.600000000000001,
        "Clarity": 95.19999999999999,
        "Conciseness": 37.2,
        "Self-Containedness": 98.2
      },
      "overall": 35.012,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 2.5 Flash Lite Preview (2025-06-17)",
    "rejection_rate": 15.0,
    "all": {
      "scores": {
        "Faithfulness": 12.8,
        "Relevance": 68.4,
        "Completeness": 18.0,
        "Clarity": 92.0,
        "Conciseness": 43.0,
        "Self-Containedness": 95.39999999999999
      },
      "overall": 34.992000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.411764705882351,
        "Relevance": 70.11764705882354,
        "Completeness": 21.176470588235293,
        "Clarity": 90.58823529411765,
        "Conciseness": 43.29411764705882,
        "Self-Containedness": 95.05882352941177
      },
      "overall": 36.09411764705882,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 235B A22B",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 14.0,
        "Relevance": 69.80000000000001,
        "Completeness": 14.399999999999999,
        "Clarity": 92.0,
        "Conciseness": 39.4,
        "Self-Containedness": 97.4
      },
      "overall": 34.972,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.141414141414142,
        "Relevance": 70.5050505050505,
        "Completeness": 14.545454545454547,
        "Clarity": 91.91919191919192,
        "Conciseness": 39.39393939393939,
        "Self-Containedness": 97.37373737373737
      },
      "overall": 35.1959595959596,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q5_0",
    "rejection_rate": 43.0,
    "all": {
      "scores": {
        "Faithfulness": 16.200000000000003,
        "Relevance": 60.8,
        "Completeness": 10.600000000000001,
        "Clarity": 96.4,
        "Conciseness": 55.8,
        "Self-Containedness": 96.4
      },
      "overall": 34.904,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 20.701754385964914,
        "Relevance": 69.12280701754386,
        "Completeness": 18.596491228070175,
        "Clarity": 94.3859649122807,
        "Conciseness": 49.824561403508774,
        "Self-Containedness": 93.68421052631578
      },
      "overall": 39.39298245614035,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.1 70B",
    "rejection_rate": 62.0,
    "all": {
      "scores": {
        "Faithfulness": 14.2,
        "Relevance": 65.6,
        "Completeness": 4.8,
        "Clarity": 94.39999999999999,
        "Conciseness": 69.2,
        "Self-Containedness": 96.19999999999999
      },
      "overall": 34.711999999999996,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.789473684210527,
        "Relevance": 64.73684210526316,
        "Completeness": 12.631578947368421,
        "Clarity": 87.89473684210526,
        "Conciseness": 51.578947368421055,
        "Self-Containedness": 91.05263157894737
      },
      "overall": 34.82105263157895,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Magistral Medium 2506 (thinking)",
    "rejection_rate": 8.0,
    "all": {
      "scores": {
        "Faithfulness": 17.2,
        "Relevance": 76.39999999999999,
        "Completeness": 17.2,
        "Clarity": 57.800000000000004,
        "Conciseness": 25.6,
        "Self-Containedness": 91.8
      },
      "overall": 34.708,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 18.043478260869566,
        "Relevance": 78.04347826086956,
        "Completeness": 18.695652173913043,
        "Clarity": 58.04347826086956,
        "Conciseness": 26.73913043478261,
        "Self-Containedness": 91.95652173913044
      },
      "overall": 35.78695652173913,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 1.5 Flash",
    "rejection_rate": 22.0,
    "all": {
      "scores": {
        "Faithfulness": 14.0,
        "Relevance": 65.6,
        "Completeness": 9.200000000000001,
        "Clarity": 97.2,
        "Conciseness": 56.0,
        "Self-Containedness": 95.39999999999999
      },
      "overall": 34.662,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.64102564102564,
        "Relevance": 74.87179487179488,
        "Completeness": 11.794871794871796,
        "Clarity": 96.92307692307692,
        "Conciseness": 61.02564102564102,
        "Self-Containedness": 95.38461538461539
      },
      "overall": 38.05384615384616,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q6_K",
    "rejection_rate": 43.0,
    "all": {
      "scores": {
        "Faithfulness": 16.4,
        "Relevance": 62.199999999999996,
        "Completeness": 8.4,
        "Clarity": 95.8,
        "Conciseness": 52.800000000000004,
        "Self-Containedness": 95.0
      },
      "overall": 34.584,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 17.894736842105264,
        "Relevance": 65.6140350877193,
        "Completeness": 14.736842105263158,
        "Clarity": 93.68421052631578,
        "Conciseness": 41.754385964912274,
        "Self-Containedness": 94.3859649122807
      },
      "overall": 36.140350877192986,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 2.5 Flash Preview (2025-05-20)",
    "rejection_rate": 13.0,
    "all": {
      "scores": {
        "Faithfulness": 13.4,
        "Relevance": 66.2,
        "Completeness": 13.200000000000001,
        "Clarity": 96.0,
        "Conciseness": 46.8,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 34.498000000000005,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.942528735632184,
        "Relevance": 68.04597701149424,
        "Completeness": 15.172413793103448,
        "Clarity": 95.86206896551724,
        "Conciseness": 46.206896551724135,
        "Self-Containedness": 95.63218390804599
      },
      "overall": 35.87816091954023,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3 70B",
    "rejection_rate": 11.0,
    "all": {
      "scores": {
        "Faithfulness": 13.200000000000001,
        "Relevance": 65.6,
        "Completeness": 12.6,
        "Clarity": 95.39999999999999,
        "Conciseness": 51.2,
        "Self-Containedness": 96.0
      },
      "overall": 34.47,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.932584269662922,
        "Relevance": 70.1123595505618,
        "Completeness": 14.157303370786515,
        "Clarity": 95.0561797752809,
        "Conciseness": 51.235955056179776,
        "Self-Containedness": 96.40449438202246
      },
      "overall": 35.97303370786517,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Nova Micro 1.0",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 9.0,
        "Relevance": 78.4,
        "Completeness": 9.399999999999999,
        "Clarity": 95.8,
        "Conciseness": 46.8,
        "Self-Containedness": 98.80000000000001
      },
      "overall": 34.368,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 9.0,
        "Relevance": 78.4,
        "Completeness": 9.399999999999999,
        "Clarity": 95.8,
        "Conciseness": 46.8,
        "Self-Containedness": 98.80000000000001
      },
      "overall": 34.368,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q4_K_S",
    "rejection_rate": 45.0,
    "all": {
      "scores": {
        "Faithfulness": 16.599999999999998,
        "Relevance": 58.8,
        "Completeness": 8.2,
        "Clarity": 95.60000000000001,
        "Conciseness": 55.599999999999994,
        "Self-Containedness": 93.0
      },
      "overall": 34.08,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 20.36363636363636,
        "Relevance": 65.81818181818181,
        "Completeness": 14.90909090909091,
        "Clarity": 92.0,
        "Conciseness": 46.18181818181818,
        "Self-Containedness": 92.36363636363636
      },
      "overall": 37.45454545454545,
      "perfect_faithfulness_pct": 1.8181818181818181
    }
  },
  {
    "model": "Llama 3.3 70B Q5_K_M",
    "rejection_rate": 47.0,
    "all": {
      "scores": {
        "Faithfulness": 15.600000000000001,
        "Relevance": 61.4,
        "Completeness": 6.0,
        "Clarity": 96.0,
        "Conciseness": 57.400000000000006,
        "Self-Containedness": 96.6
      },
      "overall": 34.016000000000005,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 19.245283018867923,
        "Relevance": 69.43396226415095,
        "Completeness": 11.320754716981131,
        "Clarity": 92.45283018867924,
        "Conciseness": 42.64150943396227,
        "Self-Containedness": 96.22641509433961
      },
      "overall": 36.928301886792454,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q4_K_M",
    "rejection_rate": 41.0,
    "all": {
      "scores": {
        "Faithfulness": 14.6,
        "Relevance": 62.400000000000006,
        "Completeness": 7.0,
        "Clarity": 98.2,
        "Conciseness": 55.599999999999994,
        "Self-Containedness": 96.8
      },
      "overall": 33.980000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.254237288135592,
        "Relevance": 67.79661016949152,
        "Completeness": 11.864406779661017,
        "Clarity": 97.6271186440678,
        "Conciseness": 44.40677966101695,
        "Self-Containedness": 96.61016949152541
      },
      "overall": 35.4,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "GLM 4 32B",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 13.4,
        "Relevance": 69.80000000000001,
        "Completeness": 11.0,
        "Clarity": 90.39999999999999,
        "Conciseness": 40,
        "Self-Containedness": 94.60000000000001
      },
      "overall": 33.93600000000001,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.535353535353536,
        "Relevance": 69.4949494949495,
        "Completeness": 11.11111111111111,
        "Clarity": 90.30303030303031,
        "Conciseness": 40,
        "Self-Containedness": 94.74747474747474
      },
      "overall": 33.95353535353536,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemma 3 27B",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 11.200000000000001,
        "Relevance": 75.6,
        "Completeness": 8.8,
        "Clarity": 96.19999999999999,
        "Conciseness": 35.0,
        "Self-Containedness": 98.2
      },
      "overall": 33.874,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.200000000000001,
        "Relevance": 75.6,
        "Completeness": 8.8,
        "Clarity": 96.19999999999999,
        "Conciseness": 35.0,
        "Self-Containedness": 98.2
      },
      "overall": 33.874,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B FP16",
    "rejection_rate": 59.0,
    "all": {
      "scores": {
        "Faithfulness": 14.8,
        "Relevance": 60.599999999999994,
        "Completeness": 7.6,
        "Clarity": 97.0,
        "Conciseness": 57.0,
        "Self-Containedness": 97.4
      },
      "overall": 33.85,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 19.51219512195122,
        "Relevance": 67.80487804878048,
        "Completeness": 18.53658536585366,
        "Clarity": 93.17073170731706,
        "Conciseness": 48.78048780487805,
        "Self-Containedness": 95.60975609756098
      },
      "overall": 38.482926829268294,
      "perfect_faithfulness_pct": 2.4390243902439024
    }
  },
  {
    "model": "Llama 4 Scout",
    "rejection_rate": 37.0,
    "all": {
      "scores": {
        "Faithfulness": 14.8,
        "Relevance": 64.2,
        "Completeness": 9.0,
        "Clarity": 86.0,
        "Conciseness": 55.4,
        "Self-Containedness": 93.6
      },
      "overall": 33.826,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 17.142857142857142,
        "Relevance": 69.20634920634922,
        "Completeness": 14.285714285714286,
        "Clarity": 79.68253968253968,
        "Conciseness": 47.61904761904762,
        "Self-Containedness": 93.33333333333334
      },
      "overall": 35.83809523809524,
      "perfect_faithfulness_pct": 1.5873015873015872
    }
  },
  {
    "model": "Claude 3.7 Sonnet (thinking)",
    "rejection_rate": 36.0,
    "all": {
      "scores": {
        "Faithfulness": 17.8,
        "Relevance": 53.4,
        "Completeness": 11.399999999999999,
        "Clarity": 98.0,
        "Conciseness": 44.6,
        "Self-Containedness": 98.0
      },
      "overall": 33.664,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 22.8125,
        "Relevance": 65.0,
        "Completeness": 17.8125,
        "Clarity": 96.875,
        "Conciseness": 42.5,
        "Self-Containedness": 97.5
      },
      "overall": 39.153125,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Claude 3.5 Haiku",
    "rejection_rate": 80.0,
    "all": {
      "scores": {
        "Faithfulness": 14.6,
        "Relevance": 60,
        "Completeness": 2.4,
        "Clarity": 99.0,
        "Conciseness": 68.60000000000001,
        "Self-Containedness": 97.6
      },
      "overall": 33.662,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 18.0,
        "Relevance": 65.0,
        "Completeness": 12.0,
        "Clarity": 97.0,
        "Conciseness": 50.0,
        "Self-Containedness": 100
      },
      "overall": 36.550000000000004,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemini 2.5 Flash",
    "rejection_rate": 20.0,
    "all": {
      "scores": {
        "Faithfulness": 13.600000000000001,
        "Relevance": 65.6,
        "Completeness": 11.0,
        "Clarity": 94.0,
        "Conciseness": 42.800000000000004,
        "Self-Containedness": 94.80000000000001
      },
      "overall": 33.64,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 16.0,
        "Relevance": 72.5,
        "Completeness": 13.75,
        "Clarity": 93.0,
        "Conciseness": 43.0,
        "Self-Containedness": 95.0
      },
      "overall": 36.545,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Command R+ (08-2024)",
    "rejection_rate": 9.0,
    "all": {
      "scores": {
        "Faithfulness": 13.799999999999999,
        "Relevance": 63.6,
        "Completeness": 13.200000000000001,
        "Clarity": 94.2,
        "Conciseness": 41.4,
        "Self-Containedness": 93.80000000000001
      },
      "overall": 33.612,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.505494505494505,
        "Relevance": 64.83516483516483,
        "Completeness": 14.505494505494505,
        "Clarity": 94.28571428571429,
        "Conciseness": 39.340659340659336,
        "Self-Containedness": 94.50549450549451
      },
      "overall": 34.29450549450549,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.1 8B",
    "rejection_rate": 76.0,
    "all": {
      "scores": {
        "Faithfulness": 11.799999999999999,
        "Relevance": 63.0,
        "Completeness": 2.8000000000000003,
        "Clarity": 96.0,
        "Conciseness": 77.8,
        "Self-Containedness": 97.6
      },
      "overall": 33.508,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.5,
        "Relevance": 50.0,
        "Completeness": 11.666666666666668,
        "Clarity": 89.16666666666666,
        "Conciseness": 49.16666666666667,
        "Self-Containedness": 93.33333333333334
      },
      "overall": 30.208333333333336,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B",
    "rejection_rate": 39.0,
    "all": {
      "scores": {
        "Faithfulness": 13.799999999999999,
        "Relevance": 59.800000000000004,
        "Completeness": 6.800000000000001,
        "Clarity": 96.19999999999999,
        "Conciseness": 61.2,
        "Self-Containedness": 98.4
      },
      "overall": 33.364000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.737704918032787,
        "Relevance": 69.50819672131148,
        "Completeness": 11.147540983606557,
        "Clarity": 94.09836065573771,
        "Conciseness": 56.065573770491795,
        "Self-Containedness": 99.01639344262294
      },
      "overall": 36.472131147540985,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.1 405B",
    "rejection_rate": 75.0,
    "all": {
      "scores": {
        "Faithfulness": 13.0,
        "Relevance": 59.0,
        "Completeness": 2.8000000000000003,
        "Clarity": 98.2,
        "Conciseness": 77.4,
        "Self-Containedness": 96.4
      },
      "overall": 33.338,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.600000000000001,
        "Relevance": 56.0,
        "Completeness": 11.200000000000001,
        "Clarity": 92.8,
        "Conciseness": 44.0,
        "Self-Containedness": 97.6
      },
      "overall": 31.840000000000003,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Nova Lite 1.0",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 7.800000000000001,
        "Relevance": 77.2,
        "Completeness": 5.4,
        "Clarity": 97.2,
        "Conciseness": 51.0,
        "Self-Containedness": 100
      },
      "overall": 33.29600000000001,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 7.800000000000001,
        "Relevance": 77.2,
        "Completeness": 5.4,
        "Clarity": 97.2,
        "Conciseness": 51.0,
        "Self-Containedness": 100
      },
      "overall": 33.29600000000001,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen2.5 72B",
    "rejection_rate": 3.0,
    "all": {
      "scores": {
        "Faithfulness": 13.200000000000001,
        "Relevance": 63.2,
        "Completeness": 12.8,
        "Clarity": 96.4,
        "Conciseness": 39.4,
        "Self-Containedness": 96.19999999999999
      },
      "overall": 33.276,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.402061855670102,
        "Relevance": 64.3298969072165,
        "Completeness": 13.195876288659793,
        "Clarity": 96.28865979381443,
        "Conciseness": 39.79381443298969,
        "Self-Containedness": 96.08247422680412
      },
      "overall": 33.680412371134025,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Acree Spotlight",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 12.6,
        "Relevance": 67.6,
        "Completeness": 10.600000000000001,
        "Clarity": 89.60000000000001,
        "Conciseness": 40.199999999999996,
        "Self-Containedness": 98.2
      },
      "overall": 33.13,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.125,
        "Relevance": 69.58333333333333,
        "Completeness": 11.041666666666668,
        "Clarity": 89.16666666666666,
        "Conciseness": 39.166666666666664,
        "Self-Containedness": 99.16666666666666
      },
      "overall": 33.76875,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "LFM 40B MoE",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 13.600000000000001,
        "Relevance": 67.2,
        "Completeness": 11.200000000000001,
        "Clarity": 83.2,
        "Conciseness": 41.2,
        "Self-Containedness": 92.2
      },
      "overall": 33.050000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.737373737373737,
        "Relevance": 67.87878787878788,
        "Completeness": 11.313131313131313,
        "Clarity": 83.03030303030303,
        "Conciseness": 40.60606060606061,
        "Self-Containedness": 92.12121212121212
      },
      "overall": 33.21212121212121,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q3_K_M",
    "rejection_rate": 45.0,
    "all": {
      "scores": {
        "Faithfulness": 14.2,
        "Relevance": 60.8,
        "Completeness": 3.5999999999999996,
        "Clarity": 94.80000000000001,
        "Conciseness": 62.599999999999994,
        "Self-Containedness": 93.80000000000001
      },
      "overall": 33.03,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.636363636363637,
        "Relevance": 67.27272727272728,
        "Completeness": 6.545454545454545,
        "Clarity": 90.90909090909092,
        "Conciseness": 56.72727272727273,
        "Self-Containedness": 92.0
      },
      "overall": 34.763636363636365,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "dots.llm1",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 13.4,
        "Relevance": 64.80000000000001,
        "Completeness": 8.6,
        "Clarity": 95.60000000000001,
        "Conciseness": 39.4,
        "Self-Containedness": 97.8
      },
      "overall": 32.922000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.46938775510204,
        "Relevance": 65.71428571428571,
        "Completeness": 8.775510204081632,
        "Clarity": 95.51020408163266,
        "Conciseness": 39.38775510204082,
        "Self-Containedness": 97.75510204081633
      },
      "overall": 33.159183673469386,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q3_K_S",
    "rejection_rate": 51.0,
    "all": {
      "scores": {
        "Faithfulness": 13.600000000000001,
        "Relevance": 59.800000000000004,
        "Completeness": 4.4,
        "Clarity": 95.19999999999999,
        "Conciseness": 64.0,
        "Self-Containedness": 93.0
      },
      "overall": 32.806000000000004,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.102040816326529,
        "Relevance": 68.57142857142857,
        "Completeness": 8.979591836734693,
        "Clarity": 90.20408163265307,
        "Conciseness": 51.42857142857143,
        "Self-Containedness": 91.83673469387756
      },
      "overall": 34.79591836734694,
      "perfect_faithfulness_pct": 2.0408163265306123
    }
  },
  {
    "model": "DeepSeek V3 (2025-03-24)",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 14.0,
        "Relevance": 64.4,
        "Completeness": 10.0,
        "Clarity": 91.4,
        "Conciseness": 36.6,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 32.796,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.141414141414142,
        "Relevance": 64.84848484848484,
        "Completeness": 10.101010101010102,
        "Clarity": 92.12121212121212,
        "Conciseness": 36.96969696969697,
        "Self-Containedness": 95.95959595959596
      },
      "overall": 33.06666666666667,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "LFM 7B",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 13.0,
        "Relevance": 71.0,
        "Completeness": 9.8,
        "Clarity": 79.6,
        "Conciseness": 35.0,
        "Self-Containedness": 96.19999999999999
      },
      "overall": 32.722,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.0,
        "Relevance": 71.0,
        "Completeness": 9.8,
        "Clarity": 79.6,
        "Conciseness": 35.0,
        "Self-Containedness": 96.19999999999999
      },
      "overall": 32.722,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Mistral Nemo",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 12.8,
        "Relevance": 70.19999999999999,
        "Completeness": 9.8,
        "Clarity": 75.4,
        "Conciseness": 42.0,
        "Self-Containedness": 97.2
      },
      "overall": 32.698,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.8,
        "Relevance": 70.19999999999999,
        "Completeness": 9.8,
        "Clarity": 75.4,
        "Conciseness": 42.0,
        "Self-Containedness": 97.2
      },
      "overall": 32.698,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemma 3 12B",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 12.2,
        "Relevance": 69.0,
        "Completeness": 7.4,
        "Clarity": 93.4,
        "Conciseness": 31.6,
        "Self-Containedness": 98.6
      },
      "overall": 32.330000000000005,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.2,
        "Relevance": 69.0,
        "Completeness": 7.4,
        "Clarity": 93.4,
        "Conciseness": 31.6,
        "Self-Containedness": 98.6
      },
      "overall": 32.330000000000005,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemma 2 27B",
    "rejection_rate": 21.0,
    "all": {
      "scores": {
        "Faithfulness": 12.6,
        "Relevance": 60.199999999999996,
        "Completeness": 10.4,
        "Clarity": 95.0,
        "Conciseness": 45.599999999999994,
        "Self-Containedness": 95.0
      },
      "overall": 32.274,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.683544303797468,
        "Relevance": 70.88607594936708,
        "Completeness": 13.164556962025316,
        "Clarity": 93.92405063291139,
        "Conciseness": 51.64556962025316,
        "Self-Containedness": 95.69620253164557
      },
      "overall": 36.21518987341772,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "DeepSeek R1 (2025-05-28)",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 12.4,
        "Relevance": 65.0,
        "Completeness": 12.2,
        "Clarity": 96.0,
        "Conciseness": 26.6,
        "Self-Containedness": 96.8
      },
      "overall": 32.262,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.448979591836736,
        "Relevance": 65.71428571428571,
        "Completeness": 12.448979591836736,
        "Clarity": 95.91836734693878,
        "Conciseness": 26.93877551020408,
        "Self-Containedness": 96.93877551020408
      },
      "overall": 32.49387755102041,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Acree Coder Large",
    "rejection_rate": 3.0,
    "all": {
      "scores": {
        "Faithfulness": 11.6,
        "Relevance": 63.8,
        "Completeness": 10.0,
        "Clarity": 91.19999999999999,
        "Conciseness": 42.800000000000004,
        "Self-Containedness": 94.80000000000001
      },
      "overall": 32.004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.546391752577318,
        "Relevance": 64.94845360824742,
        "Completeness": 10.309278350515463,
        "Clarity": 91.13402061855669,
        "Conciseness": 43.092783505154635,
        "Self-Containedness": 94.63917525773196
      },
      "overall": 32.27628865979381,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q5_1",
    "rejection_rate": 43.0,
    "all": {
      "scores": {
        "Faithfulness": 11.6,
        "Relevance": 56.2,
        "Completeness": 7.4,
        "Clarity": 96.8,
        "Conciseness": 64.80000000000001,
        "Self-Containedness": 95.8
      },
      "overall": 31.978000000000005,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.438596491228068,
        "Relevance": 64.21052631578948,
        "Completeness": 12.982456140350877,
        "Clarity": 94.3859649122807,
        "Conciseness": 54.3859649122807,
        "Self-Containedness": 95.08771929824562
      },
      "overall": 35.39298245614035,
      "perfect_faithfulness_pct": 1.7543859649122806
    }
  },
  {
    "model": "Magistral Medium 2506",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 12.6,
        "Relevance": 63.8,
        "Completeness": 13.200000000000001,
        "Clarity": 82.6,
        "Conciseness": 36.0,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 31.964,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.6,
        "Relevance": 63.8,
        "Completeness": 13.200000000000001,
        "Clarity": 82.6,
        "Conciseness": 36.0,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 31.964,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen2.5 7B",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 11.0,
        "Relevance": 65.4,
        "Completeness": 7.800000000000001,
        "Clarity": 90.39999999999999,
        "Conciseness": 42.400000000000006,
        "Self-Containedness": 96.6
      },
      "overall": 31.628000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.25,
        "Relevance": 65.0,
        "Completeness": 8.125,
        "Clarity": 90.0,
        "Conciseness": 41.875,
        "Self-Containedness": 96.45833333333334
      },
      "overall": 31.650000000000002,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Acree Virtuoso Medium V2",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 12.8,
        "Relevance": 62.199999999999996,
        "Completeness": 10.0,
        "Clarity": 88.80000000000001,
        "Conciseness": 36.800000000000004,
        "Self-Containedness": 94.39999999999999
      },
      "overall": 31.624000000000002,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.916666666666668,
        "Relevance": 62.91666666666667,
        "Completeness": 10.416666666666668,
        "Clarity": 88.33333333333334,
        "Conciseness": 36.25,
        "Self-Containedness": 94.16666666666666
      },
      "overall": 31.81666666666667,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Acree Virtuoso Large",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 13.600000000000001,
        "Relevance": 60,
        "Completeness": 8.6,
        "Clarity": 92.6,
        "Conciseness": 35.6,
        "Self-Containedness": 94.80000000000001
      },
      "overall": 31.486,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 13.541666666666668,
        "Relevance": 60.83333333333333,
        "Completeness": 8.958333333333334,
        "Clarity": 92.29166666666666,
        "Conciseness": 35.208333333333336,
        "Self-Containedness": 94.79166666666666
      },
      "overall": 31.641666666666666,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.2 3B",
    "rejection_rate": 78.0,
    "all": {
      "scores": {
        "Faithfulness": 10.8,
        "Relevance": 58.2,
        "Completeness": 0.6,
        "Clarity": 95.60000000000001,
        "Conciseness": 73.60000000000001,
        "Self-Containedness": 98.2
      },
      "overall": 31.398000000000003,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 5.454545454545454,
        "Relevance": 35.45454545454545,
        "Completeness": 2.727272727272727,
        "Clarity": 80.90909090909092,
        "Conciseness": 37.27272727272727,
        "Self-Containedness": 93.63636363636363
      },
      "overall": 21.11818181818182,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 32B",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 10.4,
        "Relevance": 63.0,
        "Completeness": 9.200000000000001,
        "Clarity": 94.60000000000001,
        "Conciseness": 37.599999999999994,
        "Self-Containedness": 95.8
      },
      "overall": 31.064000000000004,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.612244897959185,
        "Relevance": 64.28571428571429,
        "Completeness": 9.387755102040817,
        "Clarity": 94.48979591836734,
        "Conciseness": 37.142857142857146,
        "Self-Containedness": 96.3265306122449
      },
      "overall": 31.4265306122449,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "GLM Z1 32B",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 12.8,
        "Relevance": 65.6,
        "Completeness": 5.0,
        "Clarity": 89.39999999999999,
        "Conciseness": 30.0,
        "Self-Containedness": 96.0
      },
      "overall": 31.018,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.92929292929293,
        "Relevance": 66.26262626262627,
        "Completeness": 5.050505050505051,
        "Clarity": 89.29292929292929,
        "Conciseness": 30.303030303030305,
        "Self-Containedness": 95.95959595959596
      },
      "overall": 31.230303030303034,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.3 70B Q2_K",
    "rejection_rate": 42.0,
    "all": {
      "scores": {
        "Faithfulness": 12.8,
        "Relevance": 54.6,
        "Completeness": 5.0,
        "Clarity": 89.60000000000001,
        "Conciseness": 58.0,
        "Self-Containedness": 91.19999999999999
      },
      "overall": 30.648000000000003,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.482758620689655,
        "Relevance": 58.9655172413793,
        "Completeness": 8.620689655172413,
        "Clarity": 82.75862068965517,
        "Conciseness": 47.58620689655173,
        "Self-Containedness": 89.6551724137931
      },
      "overall": 31.675862068965518,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Valkyrie 49B v1",
    "rejection_rate": 22.0,
    "all": {
      "scores": {
        "Faithfulness": 11.200000000000001,
        "Relevance": 60.4,
        "Completeness": 9.0,
        "Clarity": 88.6,
        "Conciseness": 40.199999999999996,
        "Self-Containedness": 95.39999999999999
      },
      "overall": 30.618000000000002,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.051282051282051,
        "Relevance": 62.30769230769231,
        "Completeness": 11.538461538461537,
        "Clarity": 85.38461538461539,
        "Conciseness": 37.43589743589744,
        "Self-Containedness": 94.87179487179488
      },
      "overall": 31.405128205128207,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 235B A22B (thinking)",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 10.2,
        "Relevance": 60.599999999999994,
        "Completeness": 13.4,
        "Clarity": 91.0,
        "Conciseness": 30.0,
        "Self-Containedness": 98.0
      },
      "overall": 30.532,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.303030303030303,
        "Relevance": 60.2020202020202,
        "Completeness": 13.535353535353536,
        "Clarity": 90.90909090909092,
        "Conciseness": 30.1010101010101,
        "Self-Containedness": 97.97979797979798
      },
      "overall": 30.523232323232328,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Acree Blitz",
    "rejection_rate": 17.0,
    "all": {
      "scores": {
        "Faithfulness": 10.600000000000001,
        "Relevance": 53.4,
        "Completeness": 12.2,
        "Clarity": 95.39999999999999,
        "Conciseness": 45.199999999999996,
        "Self-Containedness": 93.4
      },
      "overall": 30.29,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.566265060240964,
        "Relevance": 59.75903614457831,
        "Completeness": 14.698795180722893,
        "Clarity": 94.45783132530121,
        "Conciseness": 44.81927710843373,
        "Self-Containedness": 93.73493975903614
      },
      "overall": 32.36385542168675,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 14B (thinking)",
    "rejection_rate": 3.0,
    "all": {
      "scores": {
        "Faithfulness": 10.2,
        "Relevance": 63.6,
        "Completeness": 7.6,
        "Clarity": 91.19999999999999,
        "Conciseness": 31.0,
        "Self-Containedness": 97.4
      },
      "overall": 30.154,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.309278350515463,
        "Relevance": 63.298969072164944,
        "Completeness": 7.835051546391752,
        "Clarity": 90.9278350515464,
        "Conciseness": 30.927835051546392,
        "Self-Containedness": 97.5257731958763
      },
      "overall": 30.164948453608247,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3 8B",
    "rejection_rate": 21.0,
    "all": {
      "scores": {
        "Faithfulness": 10.600000000000001,
        "Relevance": 57.199999999999996,
        "Completeness": 5.4,
        "Clarity": 93.2,
        "Conciseness": 51.2,
        "Self-Containedness": 95.39999999999999
      },
      "overall": 30.152,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.39240506329114,
        "Relevance": 60.75949367088608,
        "Completeness": 6.8354430379746836,
        "Clarity": 91.39240506329115,
        "Conciseness": 44.81012658227848,
        "Self-Containedness": 95.44303797468353
      },
      "overall": 30.906329113924052,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 30B A3B",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 11.6,
        "Relevance": 57.800000000000004,
        "Completeness": 7.6,
        "Clarity": 92.6,
        "Conciseness": 36.6,
        "Self-Containedness": 97.6
      },
      "overall": 30.12,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.458333333333332,
        "Relevance": 58.125,
        "Completeness": 6.875,
        "Clarity": 92.29166666666666,
        "Conciseness": 36.458333333333336,
        "Self-Containedness": 97.5
      },
      "overall": 29.95625,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 14B",
    "rejection_rate": 2.0,
    "all": {
      "scores": {
        "Faithfulness": 11.799999999999999,
        "Relevance": 57.599999999999994,
        "Completeness": 9.399999999999999,
        "Clarity": 90.0,
        "Conciseness": 34.0,
        "Self-Containedness": 96.4
      },
      "overall": 30.094,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.040816326530612,
        "Relevance": 58.775510204081634,
        "Completeness": 9.591836734693878,
        "Clarity": 89.79591836734693,
        "Conciseness": 33.673469387755105,
        "Self-Containedness": 96.3265306122449
      },
      "overall": 30.432653061224492,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemma 2 9B",
    "rejection_rate": 33.0,
    "all": {
      "scores": {
        "Faithfulness": 13.0,
        "Relevance": 46.2,
        "Completeness": 13.200000000000001,
        "Clarity": 96.0,
        "Conciseness": 43.0,
        "Self-Containedness": 93.80000000000001
      },
      "overall": 30.01,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.626865671641792,
        "Relevance": 60.8955223880597,
        "Completeness": 19.70149253731343,
        "Clarity": 94.62686567164178,
        "Conciseness": 47.76119402985074,
        "Self-Containedness": 94.62686567164178
      },
      "overall": 35.113432835820895,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 32B (thinking)",
    "rejection_rate": 1.0,
    "all": {
      "scores": {
        "Faithfulness": 10.2,
        "Relevance": 63.4,
        "Completeness": 7.6,
        "Clarity": 89.39999999999999,
        "Conciseness": 26.6,
        "Self-Containedness": 95.8
      },
      "overall": 29.631999999999998,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.303030303030303,
        "Relevance": 63.63636363636363,
        "Completeness": 7.6767676767676765,
        "Clarity": 89.29292929292929,
        "Conciseness": 26.666666666666664,
        "Self-Containedness": 95.75757575757576
      },
      "overall": 29.735353535353536,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Magistral Small 2506",
    "rejection_rate": 6.0,
    "all": {
      "scores": {
        "Faithfulness": 13.4,
        "Relevance": 71.39999999999999,
        "Completeness": 12.0,
        "Clarity": 46.2,
        "Conciseness": 17.8,
        "Self-Containedness": 85.8
      },
      "overall": 29.524,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 14.042553191489361,
        "Relevance": 72.5531914893617,
        "Completeness": 12.76595744680851,
        "Clarity": 47.02127659574468,
        "Conciseness": 18.72340425531915,
        "Self-Containedness": 85.95744680851064
      },
      "overall": 30.30851063829787,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Mistral Small 3.1 24B",
    "rejection_rate": 35.0,
    "all": {
      "scores": {
        "Faithfulness": 12.4,
        "Relevance": 52.400000000000006,
        "Completeness": 4.2,
        "Clarity": 95.8,
        "Conciseness": 44.2,
        "Self-Containedness": 96.4
      },
      "overall": 29.508000000000003,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 15.076923076923077,
        "Relevance": 56.307692307692314,
        "Completeness": 6.461538461538462,
        "Clarity": 93.84615384615385,
        "Conciseness": 37.53846153846154,
        "Self-Containedness": 96.92307692307692
      },
      "overall": 31.313846153846157,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Gemma 3 4B",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 8.8,
        "Relevance": 62.0,
        "Completeness": 6.6000000000000005,
        "Clarity": 94.39999999999999,
        "Conciseness": 31.6,
        "Self-Containedness": 97.6
      },
      "overall": 29.296,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 8.8,
        "Relevance": 62.0,
        "Completeness": 6.6000000000000005,
        "Clarity": 94.39999999999999,
        "Conciseness": 31.6,
        "Self-Containedness": 97.6
      },
      "overall": 29.296,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen3 30B A3B (thinking)",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 10.600000000000001,
        "Relevance": 60.4,
        "Completeness": 6.0,
        "Clarity": 86.8,
        "Conciseness": 28.0,
        "Self-Containedness": 96.6
      },
      "overall": 28.864,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.600000000000001,
        "Relevance": 60.4,
        "Completeness": 6.0,
        "Clarity": 86.8,
        "Conciseness": 28.0,
        "Self-Containedness": 96.6
      },
      "overall": 28.864,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "o1-mini",
    "rejection_rate": 13.0,
    "all": {
      "scores": {
        "Faithfulness": 11.200000000000001,
        "Relevance": 51.4,
        "Completeness": 9.8,
        "Clarity": 98.2,
        "Conciseness": 28.0,
        "Self-Containedness": 97.0
      },
      "overall": 28.828000000000003,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.64367816091954,
        "Relevance": 58.390804597701155,
        "Completeness": 11.264367816091953,
        "Clarity": 97.9310344827586,
        "Conciseness": 29.195402298850578,
        "Self-Containedness": 97.4712643678161
      },
      "overall": 31.218390804597703,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Llama 3.2 1B",
    "rejection_rate": 65.0,
    "all": {
      "scores": {
        "Faithfulness": 10.4,
        "Relevance": 47.199999999999996,
        "Completeness": 2.8000000000000003,
        "Clarity": 87.0,
        "Conciseness": 71.2,
        "Self-Containedness": 87.4
      },
      "overall": 28.32,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 6.285714285714286,
        "Relevance": 33.142857142857146,
        "Completeness": 8.0,
        "Clarity": 70.28571428571428,
        "Conciseness": 36.0,
        "Self-Containedness": 76.57142857142857
      },
      "overall": 20.634285714285717,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Qwen-Turbo",
    "rejection_rate": 23.0,
    "all": {
      "scores": {
        "Faithfulness": 10.2,
        "Relevance": 52.599999999999994,
        "Completeness": 2.4,
        "Clarity": 95.19999999999999,
        "Conciseness": 39.2,
        "Self-Containedness": 95.60000000000001
      },
      "overall": 27.818,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.168831168831169,
        "Relevance": 50.649350649350644,
        "Completeness": 3.116883116883117,
        "Clarity": 94.80519480519482,
        "Conciseness": 37.4025974025974,
        "Self-Containedness": 94.28571428571429
      },
      "overall": 27.8,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Grok 3 Mini Beta",
    "rejection_rate": 7.000000000000001,
    "all": {
      "scores": {
        "Faithfulness": 10.0,
        "Relevance": 49.800000000000004,
        "Completeness": 10.0,
        "Clarity": 94.39999999999999,
        "Conciseness": 27.400000000000002,
        "Self-Containedness": 91.8
      },
      "overall": 27.54,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.75268817204301,
        "Relevance": 51.39784946236559,
        "Completeness": 10.75268817204301,
        "Clarity": 93.97849462365592,
        "Conciseness": 27.311827956989244,
        "Self-Containedness": 91.18279569892474
      },
      "overall": 28.27956989247312,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Acree Maestro Reasoning",
    "rejection_rate": 0.0,
    "all": {
      "scores": {
        "Faithfulness": 10.8,
        "Relevance": 62.800000000000004,
        "Completeness": 8.0,
        "Clarity": 45.199999999999996,
        "Conciseness": 14.8,
        "Self-Containedness": 79.2
      },
      "overall": 25.436000000000003,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 10.8,
        "Relevance": 62.800000000000004,
        "Completeness": 8.0,
        "Clarity": 45.199999999999996,
        "Conciseness": 14.8,
        "Self-Containedness": 79.2
      },
      "overall": 25.436000000000003,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Phi 4",
    "rejection_rate": 36.0,
    "all": {
      "scores": {
        "Faithfulness": 9.6,
        "Relevance": 42.599999999999994,
        "Completeness": 0.6,
        "Clarity": 97.2,
        "Conciseness": 38.0,
        "Self-Containedness": 97.0
      },
      "overall": 25.322,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 9.6875,
        "Relevance": 37.8125,
        "Completeness": 0.9375,
        "Clarity": 95.625,
        "Conciseness": 33.4375,
        "Self-Containedness": 95.3125
      },
      "overall": 23.984375,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Cypher Alpha",
    "rejection_rate": 41.0,
    "all": {
      "scores": {
        "Faithfulness": 8.4,
        "Relevance": 32.8,
        "Completeness": 5.0,
        "Clarity": 97.0,
        "Conciseness": 54.800000000000004,
        "Self-Containedness": 85.19999999999999
      },
      "overall": 24.422,
      "perfect_faithfulness_pct": 1.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.88135593220339,
        "Relevance": 49.152542372881356,
        "Completeness": 8.47457627118644,
        "Clarity": 97.28813559322035,
        "Conciseness": 37.6271186440678,
        "Self-Containedness": 94.23728813559322
      },
      "overall": 29.423728813559322,
      "perfect_faithfulness_pct": 1.694915254237288
    }
  },
  {
    "model": "Claude 4 Sonnet",
    "rejection_rate": 42.0,
    "all": {
      "scores": {
        "Faithfulness": 11.799999999999999,
        "Relevance": 30.8,
        "Completeness": 5.2,
        "Clarity": 99.0,
        "Conciseness": 31.400000000000002,
        "Self-Containedness": 95.19999999999999
      },
      "overall": 24.39,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 12.413793103448276,
        "Relevance": 36.89655172413793,
        "Completeness": 8.96551724137931,
        "Clarity": 98.27586206896552,
        "Conciseness": 28.96551724137931,
        "Self-Containedness": 95.17241379310346
      },
      "overall": 26.34137931034483,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Claude 4 Opus",
    "rejection_rate": 43.0,
    "all": {
      "scores": {
        "Faithfulness": 10.4,
        "Relevance": 32.400000000000006,
        "Completeness": 4.2,
        "Clarity": 99.2,
        "Conciseness": 34.4,
        "Self-Containedness": 98.6
      },
      "overall": 24.226000000000003,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.228070175438596,
        "Relevance": 38.94736842105263,
        "Completeness": 7.368421052631579,
        "Clarity": 99.29824561403508,
        "Conciseness": 33.333333333333336,
        "Self-Containedness": 98.24561403508773
      },
      "overall": 26.400000000000002,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Codex Mini",
    "rejection_rate": 74.0,
    "all": {
      "scores": {
        "Faithfulness": 5.6000000000000005,
        "Relevance": 23.799999999999997,
        "Completeness": 2.4,
        "Clarity": 95.60000000000001,
        "Conciseness": 73.2,
        "Self-Containedness": 82.2
      },
      "overall": 21.994000000000003,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 9.230769230769232,
        "Relevance": 58.46153846153846,
        "Completeness": 9.230769230769232,
        "Clarity": 91.53846153846153,
        "Conciseness": 56.153846153846146,
        "Self-Containedness": 88.46153846153847
      },
      "overall": 30.5,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "Sarvam-M",
    "rejection_rate": 4.0,
    "all": {
      "scores": {
        "Faithfulness": 5.8,
        "Relevance": 39.6,
        "Completeness": 5.8,
        "Clarity": 27.400000000000002,
        "Conciseness": 13.0,
        "Self-Containedness": 72.0
      },
      "overall": 16.562,
      "perfect_faithfulness_pct": 0.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 6.041666666666666,
        "Relevance": 39.791666666666664,
        "Completeness": 6.041666666666666,
        "Clarity": 26.875,
        "Conciseness": 13.125,
        "Self-Containedness": 72.29166666666667
      },
      "overall": 16.733333333333334,
      "perfect_faithfulness_pct": 0.0
    }
  },
  {
    "model": "GLM Z1 Rumination 32B",
    "rejection_rate": 36.0,
    "all": {
      "scores": {
        "Faithfulness": 7.6,
        "Relevance": 20.2,
        "Completeness": 4.2,
        "Clarity": 23.599999999999998,
        "Conciseness": 15.8,
        "Self-Containedness": 30.0
      },
      "overall": 11.874,
      "perfect_faithfulness_pct": 2.0
    },
    "without_rejections": {
      "scores": {
        "Faithfulness": 11.875,
        "Relevance": 30.9375,
        "Completeness": 6.5625,
        "Clarity": 33.125,
        "Conciseness": 22.8125,
        "Self-Containedness": 42.8125
      },
      "overall": 17.9125,
      "perfect_faithfulness_pct": 3.125
    }
  }
]