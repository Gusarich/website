[
  {
    "model": "GPT-4.5 Preview",
    "scores": {
      "Faithfulness": 65.8,
      "Relevance": 98.4,
      "Completeness": 81.19999999999999,
      "Clarity": 99.2,
      "Conciseness": 82.8,
      "Self-Containedness": 100
    },
    "overall": 79.646,
    "perfect_faithfulness_pct": 37.0
  },
  {
    "model": "GPT-4o Search Preview",
    "scores": {
      "Faithfulness": 63.2,
      "Relevance": 81.19999999999999,
      "Completeness": 90.8,
      "Clarity": 98.2,
      "Conciseness": 53.8,
      "Self-Containedness": 100
    },
    "overall": 74.664,
    "perfect_faithfulness_pct": 12.0
  },
  {
    "model": "GPT-4.1 mini (2025-04-14)",
    "scores": {
      "Faithfulness": 53.4,
      "Relevance": 98.80000000000001,
      "Completeness": 78.0,
      "Clarity": 99.2,
      "Conciseness": 82.6,
      "Self-Containedness": 100
    },
    "overall": 73.55600000000001,
    "perfect_faithfulness_pct": 26.0
  },
  {
    "model": "GPT-4o mini Search Preview",
    "scores": {
      "Faithfulness": 60.8,
      "Relevance": 80.60000000000001,
      "Completeness": 88.80000000000001,
      "Clarity": 96.0,
      "Conciseness": 55.4,
      "Self-Containedness": 99.2
    },
    "overall": 73.038,
    "perfect_faithfulness_pct": 11.0
  },
  {
    "model": "GPT-4.1 (2025-04-14)",
    "scores": {
      "Faithfulness": 54.800000000000004,
      "Relevance": 95.39999999999999,
      "Completeness": 78.0,
      "Clarity": 99.0,
      "Conciseness": 74.80000000000001,
      "Self-Containedness": 100
    },
    "overall": 72.946,
    "perfect_faithfulness_pct": 19.0
  },
  {
    "model": "GPT-3.5 Turbo",
    "scores": {
      "Faithfulness": 46.4,
      "Relevance": 97.0,
      "Completeness": 74.0,
      "Clarity": 99.39999999999999,
      "Conciseness": 81.6,
      "Self-Containedness": 97.4
    },
    "overall": 69.19200000000001,
    "perfect_faithfulness_pct": 15.0
  },
  {
    "model": "Sonar Pro",
    "scores": {
      "Faithfulness": 57.0,
      "Relevance": 78.8,
      "Completeness": 82.6,
      "Clarity": 90.39999999999999,
      "Conciseness": 48.8,
      "Self-Containedness": 98.80000000000001
    },
    "overall": 68.986,
    "perfect_faithfulness_pct": 13.0
  },
  {
    "model": "o1",
    "scores": {
      "Faithfulness": 53.8,
      "Relevance": 90.39999999999999,
      "Completeness": 66.0,
      "Clarity": 99.2,
      "Conciseness": 69.2,
      "Self-Containedness": 98.4
    },
    "overall": 68.91,
    "perfect_faithfulness_pct": 23.0
  },
  {
    "model": "GPT-4o (2024-08-06)",
    "scores": {
      "Faithfulness": 52.323232323232325,
      "Relevance": 91.71717171717171,
      "Completeness": 66.46464646464646,
      "Clarity": 98.98989898989899,
      "Conciseness": 73.53535353535354,
      "Self-Containedness": 99.19191919191918
    },
    "overall": 68.90505050505051,
    "perfect_faithfulness_pct": 22.22222222222222
  },
  {
    "model": "o4-mini",
    "scores": {
      "Faithfulness": 45.199999999999996,
      "Relevance": 96.0,
      "Completeness": 69.0,
      "Clarity": 99.0,
      "Conciseness": 72.8,
      "Self-Containedness": 96.8
    },
    "overall": 66.89,
    "perfect_faithfulness_pct": 14.000000000000002
  },
  {
    "model": "Gemini 2.5 Pro Preview (2025-05-06)",
    "scores": {
      "Faithfulness": 48.8,
      "Relevance": 86.4,
      "Completeness": 72.0,
      "Clarity": 97.6,
      "Conciseness": 61.6,
      "Self-Containedness": 98.80000000000001
    },
    "overall": 66.308,
    "perfect_faithfulness_pct": 15.0
  },
  {
    "model": "Sonar Reasoning Pro",
    "scores": {
      "Faithfulness": 50.8,
      "Relevance": 72.8,
      "Completeness": 79.80000000000001,
      "Clarity": 94.2,
      "Conciseness": 45.199999999999996,
      "Self-Containedness": 99.0
    },
    "overall": 64.512,
    "perfect_faithfulness_pct": 4.0
  },
  {
    "model": "GPT-4.1 nano (2025-04-14)",
    "scores": {
      "Faithfulness": 40.4,
      "Relevance": 96.4,
      "Completeness": 63.8,
      "Clarity": 97.6,
      "Conciseness": 79.4,
      "Self-Containedness": 99.80000000000001
    },
    "overall": 64.328,
    "perfect_faithfulness_pct": 14.000000000000002
  },
  {
    "model": "Gemini 2.0 Flash",
    "scores": {
      "Faithfulness": 38.78787878787879,
      "Relevance": 91.11111111111111,
      "Completeness": 63.83838383838384,
      "Clarity": 97.37373737373737,
      "Conciseness": 71.91919191919192,
      "Self-Containedness": 99.79797979797979
    },
    "overall": 62.012121212121215,
    "perfect_faithfulness_pct": 12.121212121212121
  },
  {
    "model": "GPT-4",
    "scores": {
      "Faithfulness": 44.2,
      "Relevance": 84.60000000000001,
      "Completeness": 54.0,
      "Clarity": 97.8,
      "Conciseness": 76.0,
      "Self-Containedness": 96.0
    },
    "overall": 61.576,
    "perfect_faithfulness_pct": 22.0
  },
  {
    "model": "Grok 3 Beta",
    "scores": {
      "Faithfulness": 43.0,
      "Relevance": 82.0,
      "Completeness": 66.6,
      "Clarity": 97.8,
      "Conciseness": 53.8,
      "Self-Containedness": 99.60000000000001
    },
    "overall": 61.338,
    "perfect_faithfulness_pct": 6.0
  },
  {
    "model": "GPT-4o mini (2024-07-18)",
    "scores": {
      "Faithfulness": 38.6,
      "Relevance": 87.6,
      "Completeness": 64.80000000000001,
      "Clarity": 98.6,
      "Conciseness": 67.4,
      "Self-Containedness": 99.80000000000001
    },
    "overall": 61.168000000000006,
    "perfect_faithfulness_pct": 6.0
  },
  {
    "model": "Gemini 1.5 Pro",
    "scores": {
      "Faithfulness": 36.56565656565657,
      "Relevance": 87.27272727272727,
      "Completeness": 62.22222222222222,
      "Clarity": 97.17171717171718,
      "Conciseness": 69.0909090909091,
      "Self-Containedness": 99.19191919191918
    },
    "overall": 59.723232323232324,
    "perfect_faithfulness_pct": 6.0606060606060606
  },
  {
    "model": "Claude 3.5 Sonnet",
    "scores": {
      "Faithfulness": 43.0,
      "Relevance": 85.19999999999999,
      "Completeness": 45.4,
      "Clarity": 97.4,
      "Conciseness": 69.2,
      "Self-Containedness": 98.6
    },
    "overall": 59.182,
    "perfect_faithfulness_pct": 8.0
  },
  {
    "model": "Sonar Deep Research",
    "scores": {
      "Faithfulness": 39.2,
      "Relevance": 67.0,
      "Completeness": 93.6,
      "Clarity": 91.19999999999999,
      "Conciseness": 23.4,
      "Self-Containedness": 100
    },
    "overall": 58.91,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "GPT-4 Turbo",
    "scores": {
      "Faithfulness": 39.6,
      "Relevance": 81.0,
      "Completeness": 58.2,
      "Clarity": 96.19999999999999,
      "Conciseness": 56.6,
      "Self-Containedness": 99.2
    },
    "overall": 58.168,
    "perfect_faithfulness_pct": 6.0
  },
  {
    "model": "Claude 3 Opus",
    "scores": {
      "Faithfulness": 44.0,
      "Relevance": 79.2,
      "Completeness": 44.6,
      "Clarity": 98.80000000000001,
      "Conciseness": 62.0,
      "Self-Containedness": 99.0
    },
    "overall": 57.894000000000005,
    "perfect_faithfulness_pct": 13.0
  },
  {
    "model": "Gemini 2.5 Flash Preview (2025-04-17)",
    "scores": {
      "Faithfulness": 39.19191919191919,
      "Relevance": 84.04040404040404,
      "Completeness": 53.131313131313135,
      "Clarity": 96.36363636363637,
      "Conciseness": 58.18181818181819,
      "Self-Containedness": 98.3838383838384
    },
    "overall": 57.777777777777786,
    "perfect_faithfulness_pct": 9.090909090909092
  },
  {
    "model": "Claude 3.7 Sonnet",
    "scores": {
      "Faithfulness": 38.8,
      "Relevance": 78.8,
      "Completeness": 56.0,
      "Clarity": 98.2,
      "Conciseness": 49.800000000000004,
      "Self-Containedness": 98.80000000000001
    },
    "overall": 56.624,
    "perfect_faithfulness_pct": 3.0
  },
  {
    "model": "Claude 3.7 Sonnet (thinking)",
    "scores": {
      "Faithfulness": 40.8,
      "Relevance": 74.0,
      "Completeness": 54.2,
      "Clarity": 98.0,
      "Conciseness": 48.2,
      "Self-Containedness": 99.0
    },
    "overall": 56.120000000000005,
    "perfect_faithfulness_pct": 5.0
  },
  {
    "model": "Llama 3.3 70B",
    "scores": {
      "Faithfulness": 37.0,
      "Relevance": 79.0,
      "Completeness": 46.4,
      "Clarity": 96.6,
      "Conciseness": 66.39999999999999,
      "Self-Containedness": 98.80000000000001
    },
    "overall": 55.176,
    "perfect_faithfulness_pct": 9.0
  },
  {
    "model": "Llama 3.1 405B",
    "scores": {
      "Faithfulness": 35.75757575757576,
      "Relevance": 76.36363636363637,
      "Completeness": 35.75757575757576,
      "Clarity": 96.96969696969697,
      "Conciseness": 76.96969696969697,
      "Self-Containedness": 98.18181818181819
    },
    "overall": 52.92121212121212,
    "perfect_faithfulness_pct": 11.11111111111111
  },
  {
    "model": "Gemini 1.5 Flash",
    "scores": {
      "Faithfulness": 31.6,
      "Relevance": 84.39999999999999,
      "Completeness": 42.199999999999996,
      "Clarity": 97.6,
      "Conciseness": 63.4,
      "Self-Containedness": 98.0
    },
    "overall": 52.906,
    "perfect_faithfulness_pct": 8.0
  },
  {
    "model": "Llama 4 Scout",
    "scores": {
      "Faithfulness": 30.8,
      "Relevance": 83.0,
      "Completeness": 45.4,
      "Clarity": 96.4,
      "Conciseness": 64.2,
      "Self-Containedness": 97.0
    },
    "overall": 52.784000000000006,
    "perfect_faithfulness_pct": 6.0
  },
  {
    "model": "o3-mini",
    "scores": {
      "Faithfulness": 32.2,
      "Relevance": 78.60000000000001,
      "Completeness": 46.6,
      "Clarity": 98.2,
      "Conciseness": 53.2,
      "Self-Containedness": 95.39999999999999
    },
    "overall": 52.05800000000001,
    "perfect_faithfulness_pct": 5.0
  },
  {
    "model": "DeepSeek R1",
    "scores": {
      "Faithfulness": 31.8,
      "Relevance": 76.8,
      "Completeness": 53.6,
      "Clarity": 96.6,
      "Conciseness": 39.6,
      "Self-Containedness": 99.60000000000001
    },
    "overall": 51.84,
    "perfect_faithfulness_pct": 3.0
  },
  {
    "model": "GLM 4 32B",
    "scores": {
      "Faithfulness": 29.2,
      "Relevance": 85.60000000000001,
      "Completeness": 45.0,
      "Clarity": 91.19999999999999,
      "Conciseness": 50.8,
      "Self-Containedness": 97.8
    },
    "overall": 51.234,
    "perfect_faithfulness_pct": 2.0
  },
  {
    "model": "MiniMax-01",
    "scores": {
      "Faithfulness": 32.599999999999994,
      "Relevance": 76.39999999999999,
      "Completeness": 49.2,
      "Clarity": 91.19999999999999,
      "Conciseness": 40,
      "Self-Containedness": 95.60000000000001
    },
    "overall": 50.858,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Llama 4 Maverick",
    "scores": {
      "Faithfulness": 35.0,
      "Relevance": 70.0,
      "Completeness": 43.4,
      "Clarity": 86.4,
      "Conciseness": 60.599999999999994,
      "Self-Containedness": 93.0
    },
    "overall": 50.642,
    "perfect_faithfulness_pct": 11.0
  },
  {
    "model": "LFM 40B MoE",
    "scores": {
      "Faithfulness": 32.400000000000006,
      "Relevance": 75.19999999999999,
      "Completeness": 42.400000000000006,
      "Clarity": 88.4,
      "Conciseness": 53.0,
      "Self-Containedness": 86.19999999999999
    },
    "overall": 49.736000000000004,
    "perfect_faithfulness_pct": 7.000000000000001
  },
  {
    "model": "Qwen3 235B A22B",
    "scores": {
      "Faithfulness": 28.484848484848484,
      "Relevance": 77.97979797979798,
      "Completeness": 46.66666666666667,
      "Clarity": 94.54545454545455,
      "Conciseness": 42.02020202020202,
      "Self-Containedness": 98.3838383838384
    },
    "overall": 49.32525252525253,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Grok 3 Mini Beta",
    "scores": {
      "Faithfulness": 29.8,
      "Relevance": 68.8,
      "Completeness": 53.8,
      "Clarity": 95.19999999999999,
      "Conciseness": 38.6,
      "Self-Containedness": 96.19999999999999
    },
    "overall": 49.106,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "DeepSeek V3 (2025-03-24)",
    "scores": {
      "Faithfulness": 32.599999999999994,
      "Relevance": 64.4,
      "Completeness": 45.8,
      "Clarity": 89.39999999999999,
      "Conciseness": 36.2,
      "Self-Containedness": 93.6
    },
    "overall": 47.394,
    "perfect_faithfulness_pct": 2.0
  },
  {
    "model": "Qwen3 32B",
    "scores": {
      "Faithfulness": 23.232323232323232,
      "Relevance": 77.97979797979798,
      "Completeness": 41.41414141414141,
      "Clarity": 94.34343434343434,
      "Conciseness": 44.04040404040404,
      "Self-Containedness": 98.58585858585857
    },
    "overall": 46.14949494949495,
    "perfect_faithfulness_pct": 1.0101010101010102
  },
  {
    "model": "Claude 3.5 Haiku",
    "scores": {
      "Faithfulness": 27.200000000000003,
      "Relevance": 76.6,
      "Completeness": 22.799999999999997,
      "Clarity": 98.2,
      "Conciseness": 65.6,
      "Self-Containedness": 98.0
    },
    "overall": 46.07,
    "perfect_faithfulness_pct": 3.0
  },
  {
    "model": "Qwen3 14B",
    "scores": {
      "Faithfulness": 25.2,
      "Relevance": 74.4,
      "Completeness": 38.0,
      "Clarity": 93.6,
      "Conciseness": 44.6,
      "Self-Containedness": 99.0
    },
    "overall": 45.704,
    "perfect_faithfulness_pct": 1.0
  },
  {
    "model": "Qwen3 235B A22B (thinking)",
    "scores": {
      "Faithfulness": 24.0,
      "Relevance": 74.6,
      "Completeness": 38.199999999999996,
      "Clarity": 94.0,
      "Conciseness": 34.6,
      "Self-Containedness": 99.0
    },
    "overall": 44.568,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Qwen3 30B A3B",
    "scores": {
      "Faithfulness": 23.599999999999998,
      "Relevance": 73.4,
      "Completeness": 36.4,
      "Clarity": 92.4,
      "Conciseness": 44.2,
      "Self-Containedness": 97.0
    },
    "overall": 44.324,
    "perfect_faithfulness_pct": 1.0
  },
  {
    "model": "o1-mini",
    "scores": {
      "Faithfulness": 26.400000000000002,
      "Relevance": 65.0,
      "Completeness": 34.6,
      "Clarity": 95.39999999999999,
      "Conciseness": 35.2,
      "Self-Containedness": 98.0
    },
    "overall": 43.19,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "LFM 7B",
    "scores": {
      "Faithfulness": 22.599999999999998,
      "Relevance": 77.4,
      "Completeness": 33.0,
      "Clarity": 82.6,
      "Conciseness": 36.2,
      "Self-Containedness": 98.0
    },
    "overall": 42.846000000000004,
    "perfect_faithfulness_pct": 1.0
  },
  {
    "model": "Mistral Nemo",
    "scores": {
      "Faithfulness": 21.6,
      "Relevance": 74.6,
      "Completeness": 32.8,
      "Clarity": 82.0,
      "Conciseness": 40.599999999999994,
      "Self-Containedness": 97.6
    },
    "overall": 42.054,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "GLM Z1 32B",
    "scores": {
      "Faithfulness": 21.400000000000002,
      "Relevance": 72.4,
      "Completeness": 34.2,
      "Clarity": 92.4,
      "Conciseness": 34.2,
      "Self-Containedness": 97.2
    },
    "overall": 42.044000000000004,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Qwen3 32B (thinking)",
    "scores": {
      "Faithfulness": 21.6,
      "Relevance": 65.8,
      "Completeness": 37.400000000000006,
      "Clarity": 91.0,
      "Conciseness": 32.0,
      "Self-Containedness": 96.8
    },
    "overall": 41.126000000000005,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Qwen3 14B (thinking)",
    "scores": {
      "Faithfulness": 19.6,
      "Relevance": 72.4,
      "Completeness": 31.0,
      "Clarity": 93.0,
      "Conciseness": 33.4,
      "Self-Containedness": 98.2
    },
    "overall": 40.67400000000001,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Qwen3 30B A3B (thinking)",
    "scores": {
      "Faithfulness": 21.200000000000003,
      "Relevance": 68.0,
      "Completeness": 31.8,
      "Clarity": 90.8,
      "Conciseness": 34.6,
      "Self-Containedness": 97.0
    },
    "overall": 40.55200000000001,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Mistral Small 3.1 24B",
    "scores": {
      "Faithfulness": 23.4,
      "Relevance": 61.2,
      "Completeness": 23.799999999999997,
      "Clarity": 95.39999999999999,
      "Conciseness": 45.599999999999994,
      "Self-Containedness": 96.4
    },
    "overall": 39.816,
    "perfect_faithfulness_pct": 0.0
  },
  {
    "model": "Phi 4",
    "scores": {
      "Faithfulness": 20.4,
      "Relevance": 56.0,
      "Completeness": 19.0,
      "Clarity": 94.60000000000001,
      "Conciseness": 41.6,
      "Self-Containedness": 96.0
    },
    "overall": 36.214000000000006,
    "perfect_faithfulness_pct": 1.0
  },
  {
    "model": "Llama 3.2 3B",
    "scores": {
      "Faithfulness": 16.200000000000003,
      "Relevance": 59.400000000000006,
      "Completeness": 7.6,
      "Clarity": 92.6,
      "Conciseness": 64.4,
      "Self-Containedness": 94.2
    },
    "overall": 34.354000000000006,
    "perfect_faithfulness_pct": 2.0
  },
  {
    "model": "Llama 3.2 1B",
    "scores": {
      "Faithfulness": 11.313131313131313,
      "Relevance": 45.65656565656566,
      "Completeness": 5.252525252525253,
      "Clarity": 78.98989898989899,
      "Conciseness": 58.58585858585859,
      "Self-Containedness": 88.48484848484847
    },
    "overall": 27.452525252525255,
    "perfect_faithfulness_pct": 1.0101010101010102
  },
  {
    "model": "GLM Z1 Rumination 32B",
    "scores": {
      "Faithfulness": 11.6,
      "Relevance": 29.8,
      "Completeness": 13.200000000000001,
      "Clarity": 33.6,
      "Conciseness": 20.6,
      "Self-Containedness": 40.4
    },
    "overall": 18.562,
    "perfect_faithfulness_pct": 0.0
  }
]